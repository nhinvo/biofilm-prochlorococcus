snakefile: workflow/Snakefile
configfile: inputs/config.yaml


rerun-incomplete: True
latency-wait: 120
keep-going: True
keep-incomplete: False


# conda specifications
use-conda: True
conda-frontend: conda


# unlock: True  
# dry-run: True


# SLURM Specifications
cluster: 
  mkdir -p logs/{rule} &&
  sbatch
    --partition={resources.partition}
    --ntasks={resources.tasks}
    --cpus-per-task={resources.cpus_per_task}
    --mem={resources.mem}
    --time={resources.time}
    --job-name={rule}-%j
    --output="logs/{rule}/{wildcards}.out"
    --error="logs/{rule}/{wildcards}.err"


# HPC Resource Specifications 
# number of jobs/processes/samples running at once 
jobs: 50


default-resources: 
  - partition="sched_mit_chisholm"
  - time="12:00:00"  
  - mem=5000 
  - cpus_per_task=1  
  - tasks=1  

set-resources:
  # multi-threaded rules (allocate more CPUs)
  - SRA_download:cpus_per_task=2
  - run_trim_PE_local:cpus_per_task=5
  - run_trim_PE_sra:cpus_per_task=10
  - obtain_thermus_reads:cpus_per_task=5
  - kaiju_run:cpus_per_task=10
  - blast_reads:cpus_per_task=10

  # memory intensive rules
  - SRA_download:mem=50000
  - run_trim_PE_local:mem=25000
  - run_trim_PE_sra:mem=50000
  - obtain_thermus_reads:mem=50000
  - kaiju_run:mem=50000
  - blast_reads:mem=50000
  - normalize_reads:mem=100000

  # send some rules to other partitions
  - SRA_download:partition="mit_normal"
  - run_trim_PE_local:partition="sched_mit_hill"
  - run_trim_PE_sra:partition="newnodes"
  - kaiju_run:partition="newnodes"
  - kaiju_summary_taxa:partition="mit_normal"
